{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f45f5c",
   "metadata": {
    "papermill": {
     "duration": 0.003637,
     "end_time": "2025-02-05T18:28:44.737283",
     "exception": false,
     "start_time": "2025-02-05T18:28:44.733646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission Code for the Global Wheat Detection Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b509f",
   "metadata": {
    "papermill": {
     "duration": 0.002677,
     "end_time": "2025-02-05T18:28:44.743287",
     "exception": false,
     "start_time": "2025-02-05T18:28:44.740610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import necessary Libraries\n",
    "\n",
    "This notebook is based on pytroch lightning, so we need to import the necessary libraries for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9df32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:28:44.750702Z",
     "iopub.status.busy": "2025-02-05T18:28:44.750222Z",
     "iopub.status.idle": "2025-02-05T18:29:17.269815Z",
     "shell.execute_reply": "2025-02-05T18:29:17.268634Z"
    },
    "papermill": {
     "duration": 32.525899,
     "end_time": "2025-02-05T18:29:17.272102",
     "exception": false,
     "start_time": "2025-02-05T18:28:44.746203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849750fd",
   "metadata": {
    "papermill": {
     "duration": 0.003372,
     "end_time": "2025-02-05T18:29:17.278965",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.275593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define a Backbone Class\n",
    "\n",
    "We are using a Vision Transformer model from Huggingface as a backbone. The model has been pretrained on the PlanNet-300k dataset, which should contain similar details as the wheat images. We use a wrapper to load the model in a way, that it can be used as a backbone in the Faster-RCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0be9d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:29:17.287173Z",
     "iopub.status.busy": "2025-02-05T18:29:17.286421Z",
     "iopub.status.idle": "2025-02-05T18:29:17.296507Z",
     "shell.execute_reply": "2025-02-05T18:29:17.294493Z"
    },
    "papermill": {
     "duration": 0.016687,
     "end_time": "2025-02-05T18:29:17.298819",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.282132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- ViT Backbone Wrapper ---\n",
    "class ViTBackbone(nn.Module):\n",
    "    def __init__(self, model_name='janjibDEV/vit-plantnet300k', local_files_only=False):\n",
    "        \"\"\"\n",
    "        Wraps a ViT model so that it can serve as a backbone for Faster R-CNN.\n",
    "        It extracts patch embeddings and reshapes them into a spatial feature map.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load the pretrained ViT model\n",
    "        self.vit = ViTModel.from_pretrained(model_name, local_files_only=local_files_only)\n",
    "        # The hidden size of the ViT is used as the number of output channels.\n",
    "        self.out_channels = self.vit.config.hidden_size\n",
    "\n",
    "        # Retrieve expected image and patch sizes from the model configuration.\n",
    "        self.image_size = self.vit.config.image_size if hasattr(self.vit.config, 'image_size') else 224\n",
    "        self.patch_size = self.vit.config.patch_size if hasattr(self.vit.config, 'patch_size') else 16\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, channels, H, W)\n",
    "        Returns:\n",
    "            A dict mapping a feature map name (here \"0\") to a feature map tensor.\n",
    "            The feature map is obtained by:\n",
    "              - Running the image through the ViT,\n",
    "              - Removing the [CLS] token,\n",
    "              - Reshaping the remaining tokens into a 2D grid.\n",
    "        \"\"\"\n",
    "        # Enable positional encoding interpolation to handle inputs with different sizes.\n",
    "        outputs = self.vit(x, interpolate_pos_encoding=True)\n",
    "        # outputs.last_hidden_state shape: (batch_size, 1 + num_patches, hidden_size)\n",
    "        # Discard the [CLS] token (first token)\n",
    "        patch_tokens = outputs.last_hidden_state[:, 1:, :]  # (batch_size, num_patches, hidden_size)\n",
    "\n",
    "        # Calculate grid size: assumes the number of patches forms a perfect square.\n",
    "        grid_size = int(patch_tokens.shape[1] ** 0.5)\n",
    "        if grid_size * grid_size != patch_tokens.shape[1]:\n",
    "            raise ValueError(\"The number of patches is not a perfect square. Check your image and patch sizes.\")\n",
    "\n",
    "        # Reshape the tokens to a 2D feature map.\n",
    "        feature_map = patch_tokens.transpose(1, 2).reshape(x.size(0), self.out_channels, grid_size, grid_size)\n",
    "        return {\"0\": feature_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3af2c1",
   "metadata": {
    "papermill": {
     "duration": 0.003225,
     "end_time": "2025-02-05T18:29:17.305866",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.302641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Lightning Module with ViT backbone\n",
    "\n",
    "We need a class for the lightning module to train the model. It is not really necessary to have the lighning module for inference, but we kept it in here for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4b8632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:29:17.314227Z",
     "iopub.status.busy": "2025-02-05T18:29:17.313758Z",
     "iopub.status.idle": "2025-02-05T18:29:17.332542Z",
     "shell.execute_reply": "2025-02-05T18:29:17.331473Z"
    },
    "papermill": {
     "duration": 0.024994,
     "end_time": "2025-02-05T18:29:17.334377",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.309383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Lightning Module using the ViT Backbone ---\n",
    "class FasterRCNNModel_ViT(pl.LightningModule):\n",
    "    def __init__(self, num_classes=2, backbone_path='janjibDEV/vit-plantnet300k', local_files_only=False, aspect_ratios=(0.5, 1.0, 1.5), anchor_sizes=(12, 24, 36), roi_sampling_ratio=4, roi_output_size=7, base_lr_backbone=0.0001, base_lr_other=0.0001):\n",
    "        \"\"\"\n",
    "        Faster R-CNN model for object detection.\n",
    "        Option to use a transformer backbone.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Use the transformer backbone from Hugging Face.\n",
    "        backbone = ViTBackbone(model_name=backbone_path)\n",
    "        self.base_lr_backbone = base_lr_backbone\n",
    "        self.base_lr_other = base_lr_other\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # --- Anchor Generator ---\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=(anchor_sizes,),         # Single feature map level\n",
    "            aspect_ratios=(aspect_ratios,)\n",
    "        )\n",
    "\n",
    "        # --- ROI Pooler ---\n",
    "        roi_pooler = MultiScaleRoIAlign(\n",
    "            featmap_names=[\"0\"], \n",
    "            output_size=roi_output_size,\n",
    "            sampling_ratio=roi_sampling_ratio  # Increased sampling ratio for small objects.\n",
    "        )\n",
    "\n",
    "        # --- RPN and Faster R-CNN initialization ---\n",
    "        self.model = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=anchor_generator,\n",
    "            box_roi_pool=roi_pooler,\n",
    "            rpn_pre_nms_top_n_train=3000,\n",
    "            rpn_post_nms_top_n_train=1500,\n",
    "            rpn_pre_nms_top_n_test=3000,\n",
    "            rpn_post_nms_top_n_test=1500,\n",
    "            rpn_nms_thresh=0.8,\n",
    "            rpn_fg_iou_thresh=0.6,\n",
    "            rpn_bg_iou_thresh=0.4\n",
    "        )\n",
    "\n",
    "        self.all_outputs = []  # For storing outputs during validation\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = list(image for image in images)\n",
    "        targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "        outputs = self.model(images)\n",
    "        self.all_outputs.append({\"preds\": outputs, \"targets\": targets})\n",
    "        return {\"preds\": outputs, \"targets\": targets}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_aps = []\n",
    "        for batch_out in self.all_outputs:\n",
    "            preds = batch_out[\"preds\"]\n",
    "            targets = batch_out[\"targets\"]\n",
    "            for i in range(len(preds)):\n",
    "                pred_boxes  = preds[i][\"boxes\"]\n",
    "                pred_scores = preds[i][\"scores\"]\n",
    "                gt_boxes    = targets[i][\"boxes\"]\n",
    "                # Ensure that you have defined ap_one_image_across_thresholds.\n",
    "                ap_val = ap_one_image_across_thresholds(pred_boxes, pred_scores, gt_boxes)\n",
    "                all_aps.append(ap_val)\n",
    "        mean_ap = sum(all_aps) / len(all_aps) if all_aps else 0.0\n",
    "        self.log(\"val_mAP\", mean_ap, prog_bar=True)\n",
    "        self.all_outputs.clear()\n",
    "\n",
    "    def freeze_backbone(self, ratio):\n",
    "        \"\"\"\n",
    "        Freezes the first layers of the backbone. \n",
    "        Args: \n",
    "            ratio: The fraction of parameters to freeze, e.g. 0.6 -> freeze first 60% of params.\n",
    "        \"\"\"\n",
    "        # Unfreeze all parameters first.\n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        if ratio == 0.0:\n",
    "            return\n",
    "            \n",
    "        # Count layers in the backbone.\n",
    "        total_layers = 0\n",
    "        layers_list = []\n",
    "        for module in self.model.backbone.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):\n",
    "                total_layers += 1\n",
    "                layers_list.append(module)\n",
    "                \n",
    "        num_layers_to_freeze = int(total_layers * ratio)\n",
    "        \n",
    "        for i, module in enumerate(layers_list):\n",
    "            if i < num_layers_to_freeze:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Determine max steps based on trainer and dataloader.\n",
    "        max_steps = self.trainer.max_epochs * len(self.trainer.datamodule.train_dataloader())\n",
    "\n",
    "        backbone_params = []\n",
    "        other_params = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"backbone\" in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': self.base_lr_backbone},\n",
    "            {'params': other_params, 'lr': self.base_lr_other}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.0001)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[self.base_lr_backbone, self.base_lr_other],\n",
    "            total_steps=max_steps,\n",
    "            pct_start=0.02,\n",
    "            div_factor=2,\n",
    "            final_div_factor=2,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b60ef",
   "metadata": {
    "papermill": {
     "duration": 0.002896,
     "end_time": "2025-02-05T18:29:17.342884",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.339988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Class for the Test Data\n",
    "\n",
    "We create a dataset class for the test data in order to prepare the data correctly for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db089685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:29:17.350197Z",
     "iopub.status.busy": "2025-02-05T18:29:17.349795Z",
     "iopub.status.idle": "2025-02-05T18:29:17.356425Z",
     "shell.execute_reply": "2025-02-05T18:29:17.355304Z"
    },
    "papermill": {
     "duration": 0.01239,
     "end_time": "2025-02-05T18:29:17.358339",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.345949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for test images.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = v2.Compose([\n",
    "            v2.Resize((224, 224)),\n",
    "            v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        self.imgs = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = torchvision.transforms.functional.to_tensor(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return {\"image\": img, \"file_name\": self.imgs[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03067f5b",
   "metadata": {
    "papermill": {
     "duration": 0.002812,
     "end_time": "2025-02-05T18:29:17.364510",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.361698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now we actually initialize the model, that dataset and the dataloader. We set the batch size to 1 because we did not need optimal speed and the RAM would have constrained us anyway to a small batch size. This way, we kept the inference loop simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9601463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:29:17.371765Z",
     "iopub.status.busy": "2025-02-05T18:29:17.371426Z",
     "iopub.status.idle": "2025-02-05T18:31:08.230815Z",
     "shell.execute_reply": "2025-02-05T18:31:08.229580Z"
    },
    "papermill": {
     "duration": 110.865434,
     "end_time": "2025-02-05T18:31:08.232953",
     "exception": false,
     "start_time": "2025-02-05T18:29:17.367519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at /kaggle/input/dataforglobalwheatpredictionsubmission and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting image batch 1 of 10\n",
      "Predicting image batch 2 of 10\n",
      "Predicting image batch 3 of 10\n",
      "Predicting image batch 4 of 10\n",
      "Predicting image batch 5 of 10\n",
      "Predicting image batch 6 of 10\n",
      "Predicting image batch 7 of 10\n",
      "Predicting image batch 8 of 10\n",
      "Predicting image batch 9 of 10\n",
      "Predicting image batch 10 of 10\n",
      "saved results as submission.csv\n"
     ]
    }
   ],
   "source": [
    "# initialize model and load state_dict\n",
    "model = FasterRCNNModel_ViT(\n",
    "    num_classes=2,\n",
    "    backbone_path='/kaggle/input/dataforglobalwheatpredictionsubmission',\n",
    "    aspect_ratios=(0.5, 0.67, 1.0, 1.5, 2.0),\n",
    "    anchor_sizes=(12, 24, 36),\n",
    "    roi_sampling_ratio=4,\n",
    "    roi_output_size=7,\n",
    "    base_lr_backbone=0.0001,\n",
    "    base_lr_other=0.0001,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/dataforglobalwheatpredictionsubmission/faster_fcnn_vit.pth\", map_location=torch.device('cpu'), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "data_dir = \"/kaggle/input/global-wheat-detection/test\"\n",
    "test_dataset = TestDataset(data_dir)\n",
    "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "rescale_factor = 1024 / 224    # original image size / rescaled image size\n",
    "# make predictions\n",
    "predictions = []\n",
    "for i, data in enumerate(data_loader):\n",
    "    print(f\"Predicting image batch {i+1} of {len(data_loader)}\")\n",
    "    img = data[\"image\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "\n",
    "    image_id = data[\"file_name\"][0].split(\".\")[0]\n",
    "    boxes = outputs[0][\"boxes\"]\n",
    "    scores = outputs[0][\"scores\"]\n",
    "    pred_strings = []\n",
    "    for i in range(len(boxes)):\n",
    "        x_min, y_min, x_max, y_max = boxes[i]\n",
    "        x_min = int(round(x_min.item() * rescale_factor, 0))\n",
    "        y_min = int(round(y_min.item() * rescale_factor, 0))\n",
    "        x_max = int(round(x_max.item() * rescale_factor, 0))\n",
    "        y_max = int(round(y_max.item() * rescale_factor, 0))\n",
    "        score = scores[i]\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        pred_strings.append(f\"{score} {x_min} {y_min} {width} {height}\")\n",
    "\n",
    "    pred_string = \" \".join(pred_strings)\n",
    "    predictions.append({\"image_id\": image_id, \"PredictionString\": pred_string})\n",
    "\n",
    "\n",
    "# create submission dataframe\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "\n",
    "# save submission\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"saved results as submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1160143,
     "sourceId": 19989,
     "sourceType": "competition"
    },
    {
     "datasetId": 6610283,
     "sourceId": 10672254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 149.672629,
   "end_time": "2025-02-05T18:31:11.535636",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-05T18:28:41.863007",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
